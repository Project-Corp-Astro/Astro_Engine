# üöÄ Redis Implementation Plan - Astro Engine

## üìã Implementation Priority

### **Phase 1: Redis Caching (Week 1) - CRITICAL**
- ‚úÖ **Infrastructure Ready**: Docker, environment variables configured
- üîß **Code Changes Needed**: Application-level integration
- üéØ **Expected Impact**: 10x performance improvement

### **Phase 2: Performance Monitoring (Week 2)**
- Prometheus metrics integration
- Request/response time tracking
- Memory usage monitoring

### **Phase 3: Async Processing (Week 3)**
- Celery task queue for complex calculations
- Background processing for bulk operations

## üõ†Ô∏è Redis Implementation Steps

### **Step 1: Add Redis Client to Flask App**

```python
# astro_engine/app.py additions
import redis
import json
import hashlib
from functools import wraps

# Redis connection
redis_client = redis.Redis.from_url(
    os.getenv('REDIS_URL', 'redis://localhost:6379/0'),
    decode_responses=True
)

def generate_cache_key(data):
    """Generate consistent cache key from birth data"""
    cache_data = {
        'birth_date': data.get('birth_date'),
        'birth_time': data.get('birth_time'), 
        'latitude': round(float(data.get('latitude', 0)), 6),
        'longitude': round(float(data.get('longitude', 0)), 6),
        'timezone_offset': float(data.get('timezone_offset', 0))
    }
    key_string = json.dumps(cache_data, sort_keys=True)
    return hashlib.md5(key_string.encode()).hexdigest()

def cache_calculation(cache_prefix, ttl=3600):
    """Decorator for caching expensive calculations"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Get request data
            data = request.get_json()
            cache_key = f"{cache_prefix}:{generate_cache_key(data)}"
            
            # Try cache first
            try:
                cached_result = redis_client.get(cache_key)
                if cached_result:
                    return json.loads(cached_result)
            except Exception as e:
                app.logger.warning(f"Redis cache miss: {e}")
            
            # Calculate if not cached
            result = func(*args, **kwargs)
            
            # Store in cache
            try:
                redis_client.setex(
                    cache_key, 
                    ttl, 
                    json.dumps(result, default=str)
                )
            except Exception as e:
                app.logger.warning(f"Redis cache store failed: {e}")
            
            return result
        return wrapper
    return decorator
```

### **Step 2: Apply Caching to High-Impact Endpoints**

```python
# In routes/LahairiAyanmasa.py
@bp.route('/lahiri/natal', methods=['POST'])
@cache_calculation('natal_chart', ttl=86400)  # 24 hour cache
def calculate_natal():
    """Cached natal chart calculation"""
    # existing code...

@bp.route('/lahiri/navamsa', methods=['POST']) 
@cache_calculation('navamsa_d9', ttl=86400)
def calculate_navamsa():
    """Cached D9 calculation"""
    # existing code...

@bp.route('/lahiri/calculate_d10', methods=['POST'])
@cache_calculation('dashamsha_d10', ttl=86400)
def calculate_d10():
    """Cached D10 calculation"""
    # existing code...
```

### **Step 3: Cache Invalidation Strategy**

```python
# Cache management endpoints
@app.route('/cache/clear/<user_hash>', methods=['DELETE'])
def clear_user_cache(user_hash):
    """Clear all cached calculations for a user"""
    pattern = f"*:{user_hash}"
    keys = redis_client.keys(pattern)
    if keys:
        redis_client.delete(*keys)
    return jsonify({"cleared": len(keys)})

@app.route('/cache/stats', methods=['GET'])
def cache_stats():
    """Get cache performance statistics"""
    info = redis_client.info()
    return jsonify({
        'used_memory': info['used_memory_human'],
        'hit_rate': info.get('keyspace_hits', 0) / max(info.get('keyspace_misses', 0) + info.get('keyspace_hits', 0), 1),
        'total_keys': info['db0']['keys'] if 'db0' in info else 0
    })
```

## üìä Expected Performance Improvements

### **Before Redis:**
- Natal Chart: 3-5 seconds
- Divisional Charts (D2-D60): 2-4 seconds each  
- Complete Analysis: 60-120 seconds
- Concurrent Users: 5-10 max

### **After Redis:**
- Cached Results: 50-200ms
- First Calculation: Same as before
- Subsequent Requests: 95% faster
- Concurrent Users: 50-100+

## üéØ Priority Endpoints for Caching

1. **Natal Chart** - Most frequently requested
2. **Navamsha (D9)** - Essential for relationship analysis  
3. **Dashamsha (D10)** - Career analysis
4. **Dasha Periods** - Long-term predictions
5. **Transit Calculations** - Daily updates

## üìà Success Metrics

- **Response Time**: Target <200ms for cached results
- **Cache Hit Rate**: Target >70% after 1 week
- **Concurrent Users**: Support 10x more users
- **Server Load**: 50% reduction in CPU usage

## üîÑ Implementation Timeline

**Week 1**: Redis integration and basic caching
**Week 2**: Performance monitoring and optimization  
**Week 3**: Advanced features (async processing)
**Week 4**: Production deployment and monitoring

## üéØ ROI Analysis

**Development Time**: 2-3 days
**Performance Gain**: 10x improvement
**User Capacity**: 10x more concurrent users
**Server Costs**: 50% reduction needed
**User Experience**: Near-instant responses
